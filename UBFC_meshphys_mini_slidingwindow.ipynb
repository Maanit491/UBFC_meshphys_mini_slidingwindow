{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZY1lZoFZKdC",
        "outputId": "1dc37fcb-cd54-4fc0-fe99-4733b7f1356d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision torch-geometric mediapipe==0.10.14 opencv-python scipy numpy matplotlib pandas scikit-learn tensorboard tqdm\n",
        "\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU Name: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q protobuf==4.25.3 mediapipe==0.10.14\n"
      ],
      "metadata": {
        "id": "ftdhiBfVaNx0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "import scipy.signal as signal\n",
        "from scipy.fftpack import fft, fftfreq\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "fcvRZnk0aVnb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "print(\"MediaPipe OK:\", mp.__version__)\n",
        "\n",
        "# quick sanity test\n",
        "mp.solutions.pose.Pose()\n",
        "print(\"Pose loaded successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGhy9FPKaacN",
        "outputId": "b101556d-b177-4103-edf1-2e2b7d354727"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MediaPipe OK: 0.10.14\n",
            "Pose loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MiniMeshPhys: Lightweight implementation of facial rPPG using Graph Neural Networks\n",
        "Demonstrates the key concepts from the MeshPhys paper with simplified architecture.\n",
        "\n",
        "Author: rPPG Implementation\n",
        "Date: 2026\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "import scipy.signal as signal\n",
        "from scipy.fftpack import fft, fftfreq\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# 1. SPATIOTEMPORAL GRAPH CONSTRUCTION\n",
        "# ============================================================================\n",
        "class STGraphBuilder:\n",
        "    \"\"\"\n",
        "    Builds Spatiotemporal Graphs from facial video.\n",
        "    Encodes facial surface appearance and topology over time.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_mediapipe: bool = True):\n",
        "        self.use_mediapipe = use_mediapipe\n",
        "\n",
        "        if use_mediapipe:\n",
        "            self.mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "                static_image_mode=False,\n",
        "                max_num_faces=1,\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5\n",
        "            )\n",
        "            self.num_nodes = 468\n",
        "        else:\n",
        "            self.num_nodes = 852\n",
        "\n",
        "    def get_3d_mesh(self, frame: np.ndarray) -> Optional[np.ndarray]:\n",
        "        if not self.use_mediapipe:\n",
        "            return None\n",
        "\n",
        "        results = self.mp_face_mesh.process(\n",
        "            cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        )\n",
        "\n",
        "        if not results.multi_face_landmarks:\n",
        "            return None\n",
        "\n",
        "        landmarks = results.multi_face_landmarks[0]\n",
        "        return np.array([[lm.x, lm.y, lm.z] for lm in landmarks.landmark])\n",
        "\n",
        "    def extract_node_features(self, frame: np.ndarray,\n",
        "                              mesh_3d: np.ndarray) -> np.ndarray:\n",
        "        h, w, _ = frame.shape\n",
        "        node_features = []\n",
        "\n",
        "        for x, y, _ in mesh_3d:\n",
        "            px = int(x * w)\n",
        "            py = int(y * h)\n",
        "            px = np.clip(px, 0, w - 1)\n",
        "            py = np.clip(py, 0, h - 1)\n",
        "            rgb = frame[py, px, ::-1].astype(np.float32) / 255.0\n",
        "            node_features.append(rgb)\n",
        "\n",
        "        return np.array(node_features)\n",
        "\n",
        "    def build_adjacency_matrix(self, mesh_3d: np.ndarray,\n",
        "                               distance_threshold: float = 0.15) -> torch.Tensor:\n",
        "        num_nodes = len(mesh_3d)\n",
        "        adjacency = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
        "\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(i + 1, num_nodes):\n",
        "                if np.linalg.norm(mesh_3d[i] - mesh_3d[j]) < distance_threshold:\n",
        "                    adjacency[i, j] = 1.0\n",
        "                    adjacency[j, i] = 1.0\n",
        "\n",
        "        np.fill_diagonal(adjacency, 1.0)\n",
        "        return torch.from_numpy(adjacency)\n",
        "\n",
        "    def build_stgraph(self,\n",
        "                      video_path: str,\n",
        "                      skip_frames: int = 1,\n",
        "                      normalize: bool = True,\n",
        "                      max_frames: Optional[int] = 450,\n",
        "                      start_frame: int = 0) -> Dict:\n",
        "        \"\"\"\n",
        "        Build Spatiotemporal Graph from a sliding window of the video.\n",
        "        \"\"\"\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        print(f\"Processing frames {start_frame} to \"\n",
        "              f\"{'end' if max_frames is None else start_frame + max_frames}\")\n",
        "\n",
        "        node_features_list = []\n",
        "        adjacency = None\n",
        "        processed = 0\n",
        "        frame_idx = start_frame\n",
        "\n",
        "        while True:\n",
        "            if max_frames is not None and processed >= max_frames:\n",
        "                break\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_idx % skip_frames != 0:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            mesh_3d = self.get_3d_mesh(frame)\n",
        "            if mesh_3d is None:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            node_features = self.extract_node_features(frame, mesh_3d)\n",
        "            node_features_list.append(node_features)\n",
        "            processed += 1\n",
        "\n",
        "            if adjacency is None:\n",
        "                adjacency = self.build_adjacency_matrix(mesh_3d)\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if not node_features_list:\n",
        "            raise ValueError(\"No frames processed, face not detected\")\n",
        "\n",
        "        node_features = torch.from_numpy(\n",
        "            np.array(node_features_list)\n",
        "        ).float()\n",
        "\n",
        "        if normalize:\n",
        "            mean = node_features.mean(dim=(0, 1), keepdim=True)\n",
        "            std = node_features.std(dim=(0, 1), keepdim=True)\n",
        "            node_features = (node_features - mean) / (std + 1e-5)\n",
        "\n",
        "        return {\n",
        "            \"node_features\": node_features,\n",
        "            \"adjacency\": adjacency,\n",
        "            \"fps\": fps,\n",
        "            \"num_frames\": node_features.shape[0]\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. GRAPH NEURAL NETWORK MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class MultiKernelTemporalConv(nn.Module):\n",
        "    \"\"\"Multi-kernel temporal convolution for adaptive temporal modeling.\n",
        "\n",
        "    Input:  x  [T, N, C_in]  (time, nodes, channels)\n",
        "    Output: out [T, N, C_out]\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 kernels: List[int] = [3, 5, 9]):\n",
        "        super().__init__()\n",
        "        self.kernels = kernels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Parallel temporal convolutions over time dimension\n",
        "        # Conv1d expects [batch, channels, time] → we use batch = nodes\n",
        "        self.temp_convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels, out_channels, kernel,\n",
        "                      padding=kernel // 2, bias=True)\n",
        "            for kernel in kernels\n",
        "        ])\n",
        "\n",
        "        # Gating mechanism: combines K kernel branches into one\n",
        "        # Input:  [N, C_out * K] (after global pooling)\n",
        "        # Output: [N, K] (kernel weights per node)\n",
        "        self.gating_mlp = nn.Sequential(\n",
        "            nn.Linear(out_channels * len(kernels), out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_channels, len(kernels)),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [T, N, C_in] node features\n",
        "\n",
        "        Returns:\n",
        "            [T, N, C_out] fused temporal features\n",
        "        \"\"\"\n",
        "        T, N, C = x.shape                         # [T, N, C_in]\n",
        "        # Conv1d over time: [N, C_in, T]\n",
        "        x_perm = x.permute(1, 2, 0)               # [N, C_in, T]\n",
        "\n",
        "        # Apply parallel temporal convolutions\n",
        "        branch_outputs = []\n",
        "        for conv in self.temp_convs:\n",
        "            out = conv(x_perm)                    # [N, C_out, T]\n",
        "            branch_outputs.append(out)\n",
        "\n",
        "        # Stack along new branch dimension: [N, K, C_out, T]\n",
        "        stacked = torch.stack(branch_outputs, dim=1)\n",
        "\n",
        "        # For gating, first concatenate along channel dim: [N, C_out*K, T]\n",
        "        combined = torch.cat(branch_outputs, dim=1)        # [N, C_out*K, T]\n",
        "        pooled = combined.mean(dim=2)                      # [N, C_out*K]\n",
        "\n",
        "        # Compute kernel weights per node: [N, K]\n",
        "        gates = self.gating_mlp(pooled)                    # [N, K]\n",
        "        gates = gates.view(N, self.kernels.__len__(), 1, 1)  # [N, K, 1, 1]\n",
        "\n",
        "        # Apply gating and sum over kernels\n",
        "        gated = (stacked * gates).sum(dim=1)               # [N, C_out, T]\n",
        "\n",
        "        # Back to [T, N, C_out]\n",
        "        out = gated.permute(2, 0, 1)                       # [T, N, C_out]\n",
        "        return out\n",
        "\n",
        "\n",
        "class MiniMeshPhys(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight spatiotemporal graph convolutional network for facial rPPG.\n",
        "\n",
        "    Architecture:\n",
        "    - Multi-kernel temporal convolutions (adaptive temporal patterns)\n",
        "    - Spatial graph convolutions (surface-aligned modeling)\n",
        "    - Progressive spatial pooling (efficient computation)\n",
        "    - Linear prediction head (waveform estimation)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_nodes: int = 468, hidden_dim: int = 32,\n",
        "                 depth: int = 3, kernels: List[int] = [3, 5, 9]):\n",
        "        super().__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Initial projection\n",
        "        self.input_proj = nn.Linear(3, hidden_dim)\n",
        "\n",
        "        # Spatiotemporal blocks\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            in_ch = hidden_dim if i == 0 else hidden_dim * (i)\n",
        "            out_ch = hidden_dim * (i + 1)\n",
        "\n",
        "            self.layers.append(nn.ModuleDict({\n",
        "                'mktc': MultiKernelTemporalConv(in_ch, out_ch, kernels),\n",
        "                'gcn': GCNConv(out_ch, out_ch),\n",
        "                'norm': nn.BatchNorm1d(out_ch)\n",
        "            }))\n",
        "\n",
        "        # Output projection to waveform\n",
        "        final_dim = hidden_dim * depth\n",
        "        self.waveform_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_features: torch.Tensor,\n",
        "                edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            node_features: [T, N, 3] RGB values\n",
        "            edge_index: [2, E] edge indices\n",
        "\n",
        "        Returns:\n",
        "            waveform: [T] predicted PPG signal\n",
        "        \"\"\"\n",
        "        T, N, _ = node_features.shape\n",
        "\n",
        "        # Project input\n",
        "        x = self.input_proj(node_features)  # [T, N, hidden_dim]\n",
        "\n",
        "        # Spatiotemporal processing\n",
        "        for layer in self.layers:\n",
        "            # Temporal convolution\n",
        "            x_temporal = layer['mktc'](x)  # [T, N, C]\n",
        "\n",
        "            # Spatial convolution per timeframe\n",
        "            x_spatial_list = []\n",
        "            for t in range(T):\n",
        "                x_t = x_temporal[t]  # [N, C]\n",
        "                x_t = layer['gcn'](x_t, edge_index)  # [N, C]\n",
        "                x_t = layer['norm'](x_t)  # [N, C]\n",
        "                x_t = F.relu(x_t)\n",
        "                x_spatial_list.append(x_t)\n",
        "\n",
        "            x = torch.stack(x_spatial_list, dim=0)  # [T, N, C]\n",
        "\n",
        "        # Global average pooling\n",
        "        x = x.mean(dim=1)  # [T, C]\n",
        "\n",
        "        # Predict waveform\n",
        "        waveform = self.waveform_head(x).squeeze(-1)  # [T]\n",
        "\n",
        "        return waveform\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DATASET HANDLING\n",
        "# ============================================================================\n",
        "\n",
        "class rPPGDataset:\n",
        "    \"\"\"\n",
        "    rPPG dataset handler for UBFC-style data.\n",
        "    Loads video and Raw BVP (Blood Volume Pulse) ground truth from text file.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, video_path: str, ppg_path: Optional[str] = None):\n",
        "        self.video_path = video_path\n",
        "        self.ppg_path = ppg_path\n",
        "        self.stgraph_builder = STGraphBuilder()\n",
        "\n",
        "    def load_ppg_ground_truth(self,\n",
        "                              num_frames: Optional[int] = None) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load Raw BVP ground truth from UBFC text file.\n",
        "        Supports: .txt, .csv, .npy\n",
        "\n",
        "        Args:\n",
        "            num_frames: if provided, resample BVP to match video frame count\n",
        "\n",
        "        Returns:\n",
        "            1D numpy array of BVP values\n",
        "        \"\"\"\n",
        "        if self.ppg_path is None:\n",
        "            return None\n",
        "\n",
        "        path = Path(self.ppg_path)\n",
        "        if not path.exists():\n",
        "            return None\n",
        "\n",
        "        bvp = None\n",
        "\n",
        "        # NPY\n",
        "        if path.suffix == \".npy\":\n",
        "            bvp = np.load(path).flatten()\n",
        "\n",
        "        # TXT / CSV (UBFC style)\n",
        "        elif path.suffix in [\".txt\", \".csv\"]:\n",
        "            values = []\n",
        "            with open(path, \"r\") as f:\n",
        "                for v in f.read().replace(\",\", \" \").split():\n",
        "                    try:\n",
        "                        values.append(float(v))\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "            if values:\n",
        "                bvp = np.array(values, dtype=np.float32)\n",
        "\n",
        "        if bvp is None or len(bvp) == 0:\n",
        "            return None\n",
        "\n",
        "        # Resample BVP to match window length\n",
        "        if num_frames is not None and len(bvp) != num_frames:\n",
        "            bvp = signal.resample(bvp, num_frames)\n",
        "\n",
        "        # Remove DC component (important for FFT)\n",
        "        bvp = bvp - np.mean(bvp)\n",
        "\n",
        "        return bvp.astype(np.float32)\n",
        "\n",
        "    def get_stgraph(self,\n",
        "                    max_frames: int = 450,\n",
        "                    start_frame: int = 0) -> Dict:\n",
        "        \"\"\"\n",
        "        Build spatiotemporal graph from a sliding window of the video.\n",
        "\n",
        "        Args:\n",
        "            max_frames: number of frames in the window\n",
        "            start_frame: starting frame index of the window\n",
        "        \"\"\"\n",
        "        return self.stgraph_builder.build_stgraph(\n",
        "            self.video_path,\n",
        "            max_frames=max_frames,\n",
        "            start_frame=start_frame\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def create_dummy_ppg(num_frames: int,\n",
        "                         fps: float,\n",
        "                         heart_rate: float = 70.0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create synthetic PPG signal (debugging only).\n",
        "        \"\"\"\n",
        "        t = np.arange(num_frames) / fps\n",
        "        freq = heart_rate / 60.0\n",
        "        ppg = np.sin(2 * np.pi * freq * t)\n",
        "        ppg += 0.1 * np.random.randn(num_frames)\n",
        "        return ppg\n",
        "\n",
        "# ============================================================================\n",
        "# 4. LOSS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "class rPPGLosses:\n",
        "    \"\"\"\n",
        "    Losses for rPPG training based on MeshPhys paper.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def morphological_correlation(pred: torch.Tensor,\n",
        "                                 target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pearson correlation loss.\n",
        "        Captures morphological similarity regardless of scale.\n",
        "        \"\"\"\n",
        "        pred = pred - pred.mean()\n",
        "        target = target - target.mean()\n",
        "\n",
        "        numerator = (pred * target).sum()\n",
        "        denominator = torch.sqrt((pred ** 2).sum() * (target ** 2).sum())\n",
        "\n",
        "        correlation = numerator / (denominator + 1e-7)\n",
        "        return 1.0 - correlation\n",
        "\n",
        "    @staticmethod\n",
        "    def temporal_derivative_loss(pred: torch.Tensor,\n",
        "                                target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        First-order derivative correlation.\n",
        "        Captures temporal dynamics.\n",
        "        \"\"\"\n",
        "        pred_diff = pred[1:] - pred[:-1]\n",
        "        target_diff = target[1:] - target[:-1]\n",
        "        return rPPGLosses.morphological_correlation(pred_diff, target_diff)\n",
        "\n",
        "    @staticmethod\n",
        "    def phase_shift_robust_loss(pred: torch.Tensor,\n",
        "                               target: torch.Tensor,\n",
        "                               max_shift: int = 50,\n",
        "                               beta: float = 10.0) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Loss that handles phase shifts between video and PPG.\n",
        "        Averages over circular shifts.\n",
        "        \"\"\"\n",
        "        min_loss = float('inf')\n",
        "\n",
        "        for shift in range(-max_shift, max_shift):\n",
        "            if shift > 0:\n",
        "                pred_shifted = pred[shift:]\n",
        "                target_shifted = target[:-shift]\n",
        "            elif shift < 0:\n",
        "                pred_shifted = pred[:shift]\n",
        "                target_shifted = target[-shift:]\n",
        "            else:\n",
        "                pred_shifted = pred\n",
        "                target_shifted = target\n",
        "\n",
        "            if len(pred_shifted) == 0:\n",
        "                continue\n",
        "\n",
        "            loss = rPPGLosses.morphological_correlation(pred_shifted, target_shifted)\n",
        "            min_loss = min(min_loss.item() if isinstance(min_loss, torch.Tensor) else min_loss,\n",
        "                          loss.item())\n",
        "\n",
        "        return torch.tensor(min_loss, device=pred.device, dtype=pred.dtype)\n",
        "\n",
        "    @staticmethod\n",
        "    def composite_loss(pred: torch.Tensor, target: torch.Tensor,\n",
        "                      use_phase_shift: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Combined loss from MeshPhys paper.\n",
        "        \"\"\"\n",
        "        l_corr = rPPGLosses.morphological_correlation(pred, target)\n",
        "        l_diff = rPPGLosses.temporal_derivative_loss(pred, target)\n",
        "\n",
        "        loss = l_corr + 0.5 * l_diff\n",
        "\n",
        "        if use_phase_shift:\n",
        "            l_phase = rPPGLosses.phase_shift_robust_loss(pred, target)\n",
        "            loss += 0.1 * l_phase\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. SIGNAL PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class SignalProcessor:\n",
        "    \"\"\"\n",
        "    Processing and analysis of PPG / BVP signals.\n",
        "\n",
        "    Assumes:\n",
        "    - Signal is already resampled to video FPS\n",
        "    - One sample per video frame\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_heart_rate(signal_data: np.ndarray,\n",
        "                           fps: float,\n",
        "                           band: Tuple[float, float] = (0.7, 4.0)) -> float:\n",
        "        \"\"\"\n",
        "        Extract heart rate (BPM) from PPG/BVP using frequency-domain analysis.\n",
        "        \"\"\"\n",
        "\n",
        "        if signal_data is None or len(signal_data) < int(3 * fps):\n",
        "            return float(\"nan\")\n",
        "\n",
        "        # Remove DC component\n",
        "        signal_data = signal_data - np.mean(signal_data)\n",
        "\n",
        "        # Normalized band limits (Nyquist-safe)\n",
        "        low = band[0] / (fps / 2)\n",
        "        high = band[1] / (fps / 2)\n",
        "        high = min(high, 0.99)\n",
        "\n",
        "        # Band-pass filter\n",
        "        sos = signal.butter(\n",
        "            4, [low, high],\n",
        "            btype=\"band\",\n",
        "            output=\"sos\"\n",
        "        )\n",
        "        filtered = signal.sosfiltfilt(sos, signal_data)\n",
        "\n",
        "        # Windowing to reduce spectral leakage\n",
        "        windowed = filtered * np.hanning(len(filtered))\n",
        "\n",
        "        # One-sided FFT → Power spectrum\n",
        "        fft_vals = np.abs(np.fft.rfft(windowed))\n",
        "        psd = fft_vals ** 2\n",
        "        freqs = np.fft.rfftfreq(len(windowed), d=1.0 / fps)\n",
        "\n",
        "        # Physiological frequency mask\n",
        "        mask = (freqs >= band[0]) & (freqs <= band[1])\n",
        "        if not np.any(mask):\n",
        "            return float(\"nan\")\n",
        "\n",
        "        # Dominant frequency\n",
        "        dominant_freq = freqs[mask][np.argmax(psd[mask])]\n",
        "\n",
        "        return float(dominant_freq * 60.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def snr_weighting(signal_data: np.ndarray,\n",
        "                      fps: float,\n",
        "                      heart_rate: float) -> float:\n",
        "        \"\"\"\n",
        "        Approximate SNR around heart-rate frequency.\n",
        "        Used for reliability weighting.\n",
        "        \"\"\"\n",
        "\n",
        "        if signal_data is None or len(signal_data) < int(3 * fps):\n",
        "            return 0.1\n",
        "\n",
        "        signal_data = signal_data - np.mean(signal_data)\n",
        "\n",
        "        low = 0.7 / (fps / 2)\n",
        "        high = min(4.0 / (fps / 2), 0.99)\n",
        "\n",
        "        sos = signal.butter(\n",
        "            4, [low, high],\n",
        "            btype=\"band\",\n",
        "            output=\"sos\"\n",
        "        )\n",
        "        filtered = signal.sosfiltfilt(sos, signal_data)\n",
        "\n",
        "        windowed = filtered * np.hanning(len(filtered))\n",
        "        psd = np.abs(np.fft.rfft(windowed)) ** 2\n",
        "        freqs = np.fft.rfftfreq(len(windowed), d=1.0 / fps)\n",
        "\n",
        "        hr_hz = heart_rate / 60.0\n",
        "        signal_band = (freqs >= hr_hz - 0.5) & (freqs <= hr_hz + 0.5)\n",
        "\n",
        "        signal_power = psd[signal_band].sum()\n",
        "        total_power = psd.sum() + 1e-8\n",
        "\n",
        "        snr = signal_power / total_power\n",
        "        return float(np.clip(snr, 0.1, 1.0))\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Training loop for MiniMeshPhys.\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, device: str = 'cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "        self.history = {'loss': [], 'hr_error': []}\n",
        "\n",
        "    def train_step(self, node_features: torch.Tensor,\n",
        "                  edge_index: torch.Tensor,\n",
        "                  ppg_ground_truth: torch.Tensor) -> float:\n",
        "        \"\"\"Single training step.\"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        pred_waveform = self.model(node_features, edge_index)\n",
        "\n",
        "        # Loss\n",
        "        loss = rPPGLosses.composite_loss(pred_waveform, ppg_ground_truth,\n",
        "                                        use_phase_shift=False)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train_epoch(self, dataset: rPPGDataset, num_epochs: int = 50):\n",
        "        \"\"\"Train for multiple epochs.\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        # Get data\n",
        "        stgraph = dataset.get_stgraph()\n",
        "        node_features = stgraph['node_features'].to(self.device)\n",
        "        adjacency = stgraph['adjacency'].to(self.device)\n",
        "        fps = stgraph['fps']\n",
        "\n",
        "        # Get or create ground truth\n",
        "        ppg_gt = dataset.load_ppg_ground_truth()\n",
        "        if ppg_gt is None:\n",
        "            raise FileNotFoundError(\n",
        "        \"Ground truth BVP file could not be loaded. \"\n",
        "        \"Provide a valid UBFC Raw BVP text file.\"\n",
        "    )\n",
        "\n",
        "        ppg_gt = torch.from_numpy(ppg_gt[:node_features.shape[0]]).float().to(self.device)\n",
        "\n",
        "        # Convert adjacency to edge_index\n",
        "        edge_index = adjacency.nonzero(as_tuple=True)\n",
        "        edge_index = torch.stack(edge_index).to(self.device)\n",
        "\n",
        "        # Training loop\n",
        "        print(\"\\nTraining MiniMeshPhys\")\n",
        "        print(\"-\" * 50)\n",
        "        for epoch in range(num_epochs):\n",
        "            loss = self.train_step(node_features, edge_index, ppg_gt)\n",
        "\n",
        "            # Extract heart rate\n",
        "            with torch.no_grad():\n",
        "                pred_waveform = self.model(node_features, edge_index)\n",
        "                pred_ppg = pred_waveform.cpu().numpy()\n",
        "\n",
        "            pred_hr = SignalProcessor.extract_heart_rate(pred_ppg, fps)\n",
        "            gt_hr = SignalProcessor.extract_heart_rate(ppg_gt.cpu().numpy(), fps)\n",
        "            hr_error = abs(pred_hr - gt_hr)\n",
        "\n",
        "            self.history['loss'].append(loss)\n",
        "            self.history['hr_error'].append(hr_error)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | HR Error: {hr_error:.2f} BPM \"\n",
        "                      f\"(Pred: {pred_hr:.1f}, GT: {gt_hr:.1f})\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        return node_features, edge_index, ppg_gt\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save model checkpoint.\"\"\"\n",
        "        torch.save({\n",
        "            'model_state': self.model.state_dict(),\n",
        "            'history': self.history\n",
        "        }, path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load(self, path: str):\n",
        "        \"\"\"Load model checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state'])\n",
        "        self.history = checkpoint['history']\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. INFERENCE & VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "class Visualizer:\n",
        "    \"\"\"Visualization utilities.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_history(history: Dict, save_path: Optional[str] = None):\n",
        "        \"\"\"Plot training metrics.\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Loss\n",
        "        axes[0].plot(history['loss'], label='Correlation Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_title('Training Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # HR Error\n",
        "        axes[1].plot(history['hr_error'], label='Heart Rate Error')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('HR Error (BPM)')\n",
        "        axes[1].set_title('Heart Rate Estimation Error')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
        "            print(f\"Figure saved to {save_path}\")\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_waveform_comparison(pred_waveform: np.ndarray,\n",
        "                                 gt_waveform: np.ndarray,\n",
        "                                 fps: float,\n",
        "                                 save_path: Optional[str] = None):\n",
        "        \"\"\"Compare predicted vs ground truth PPG.\"\"\"\n",
        "        time = np.arange(len(pred_waveform)) / fps\n",
        "\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "        # Time domain\n",
        "        axes[0].plot(time, pred_waveform, label='Predicted', alpha=0.7, linewidth=1)\n",
        "        axes[0].plot(time, gt_waveform, label='Ground Truth', alpha=0.7, linewidth=1)\n",
        "        axes[0].set_xlabel('Time (s)')\n",
        "        axes[0].set_ylabel('PPG Amplitude')\n",
        "        axes[0].set_title('PPG Waveform Comparison (Time Domain)')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Frequency domain\n",
        "        pred_fft = np.abs(fft(pred_waveform))\n",
        "        gt_fft = np.abs(fft(gt_waveform))\n",
        "        freqs = fftfreq(len(pred_waveform), 1 / fps)\n",
        "\n",
        "        # Only show physiological range (0.7 - 4 Hz)\n",
        "        mask = (freqs > 0.7) & (freqs < 4.0)\n",
        "\n",
        "        axes[1].semilogy(freqs[mask], pred_fft[mask], label='Predicted', alpha=0.7, linewidth=1)\n",
        "        axes[1].semilogy(freqs[mask], gt_fft[mask], label='Ground Truth', alpha=0.7, linewidth=1)\n",
        "        axes[1].set_xlabel('Frequency (Hz)')\n",
        "        axes[1].set_ylabel('Power (dB)')\n",
        "        axes[1].set_title('PPG Power Spectrum (Frequency Domain)')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3, which='both')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
        "            print(f\"Figure saved to {save_path}\")\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_activation_maps(node_features: torch.Tensor,\n",
        "                             model: nn.Module,\n",
        "                             edge_index: torch.Tensor,\n",
        "                             fps: float,\n",
        "                             save_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Visualize node activations over time.\n",
        "        Shows which facial regions are important for rPPG.\n",
        "        \"\"\"\n",
        "        # ✅ ensure everything is on the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        node_features = node_features.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Sample some nodes (can't visualize all 468)\n",
        "            sample_nodes = [0, 50, 100, 150, 200]\n",
        "            sample_node_features = node_features[:, sample_nodes, :]  # kept if you later use it\n",
        "\n",
        "            # Reconstruct intermediate activations\n",
        "            x = model.input_proj(node_features)  # x: [T, N, C]\n",
        "\n",
        "            fig, axes = plt.subplots(len(sample_nodes), 1,\n",
        "                                     figsize=(12, 3 * len(sample_nodes)))\n",
        "            if len(sample_nodes) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            time = np.arange(x.shape[0]) / fps\n",
        "\n",
        "            for idx, node_id in enumerate(sample_nodes):\n",
        "                # Get activations for this node across time\n",
        "                activations = x[:, node_id, :].detach().cpu().numpy()\n",
        "\n",
        "                axes[idx].plot(time, activations)\n",
        "                axes[idx].set_ylabel(f'Activation Node {node_id}')\n",
        "                axes[idx].set_title(f'Node {node_id} Activation Over Time')\n",
        "                axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "                if idx < len(sample_nodes) - 1:\n",
        "                    axes[idx].set_xlabel('')\n",
        "\n",
        "            axes[-1].set_xlabel('Time (s)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            if save_path:\n",
        "                plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
        "                print(f\"Figure saved to {save_path}\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"UBFC evaluation pipeline using sliding-window processing\"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MiniMeshPhys: UBFC Facial rPPG Evaluation (Sliding Window)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # ================= USER PATHS =================\n",
        "    VIDEO_PATH = \"/content/ubfc_video.mp4\"    # change this\n",
        "    BVP_PATH   = \"/content/ground_truth.txt\"  # change this\n",
        "\n",
        "    WINDOW_SIZE = 450   # frames (~15s at 30 FPS)\n",
        "    STRIDE = 150        # overlap\n",
        "    EPOCHS = 50\n",
        "    # =============================================\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"\\nDevice: {device}\")\n",
        "\n",
        "    # Create model\n",
        "    model = MiniMeshPhys(num_nodes=468, hidden_dim=32, depth=3).to(device)\n",
        "    print(f\"\\nModel created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "    # Dataset\n",
        "    dataset = rPPGDataset(\n",
        "        video_path=VIDEO_PATH,\n",
        "        ppg_path=BVP_PATH\n",
        "    )\n",
        "\n",
        "    # Get total video frames\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(\"Cannot open video file\")\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"\\nTotal video frames: {total_frames}\")\n",
        "    print(f\"Window size: {WINDOW_SIZE}, Stride: {STRIDE}\")\n",
        "\n",
        "    # ================= TRAINING (single window) =================\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Training MiniMeshPhys on first window only\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Train on the first window only\n",
        "    stgraph = dataset.get_stgraph(\n",
        "        max_frames=WINDOW_SIZE,\n",
        "        start_frame=0\n",
        "    )\n",
        "\n",
        "    node_features = stgraph[\"node_features\"].to(device)\n",
        "    adjacency = stgraph[\"adjacency\"].to(device)\n",
        "    fps = stgraph[\"fps\"]\n",
        "\n",
        "    ppg_gt = dataset.load_ppg_ground_truth(num_frames=WINDOW_SIZE)\n",
        "    if ppg_gt is None:\n",
        "        raise FileNotFoundError(\"Ground truth BVP file could not be loaded\")\n",
        "\n",
        "    ppg_gt = torch.from_numpy(ppg_gt).float().to(device)\n",
        "\n",
        "    edge_index = adjacency.nonzero(as_tuple=True)\n",
        "    edge_index = torch.stack(edge_index).long().to(device)\n",
        "\n",
        "    trainer = Trainer(model, device=device)\n",
        "    trainer.model.train()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = trainer.train_step(node_features, edge_index, ppg_gt)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                pred = trainer.model(node_features, edge_index)\n",
        "                pred_hr = SignalProcessor.extract_heart_rate(pred.cpu().numpy(), fps)\n",
        "                gt_hr = SignalProcessor.extract_heart_rate(ppg_gt.cpu().numpy(), fps)\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | \"\n",
        "                f\"Pred HR: {pred_hr:.1f} BPM, GT HR: {gt_hr:.1f} BPM\"\n",
        "            )\n",
        "\n",
        "    # ================= SLIDING-WINDOW EVALUATION =================\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Running sliding-window evaluation on full video\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    trainer.model.eval()\n",
        "\n",
        "    all_pred_hr = []\n",
        "    all_gt_hr = []\n",
        "\n",
        "    start = 0\n",
        "    window_id = 1\n",
        "\n",
        "    while start + WINDOW_SIZE <= total_frames:\n",
        "        print(f\"\\nWindow {window_id}: frames {start} to {start + WINDOW_SIZE}\")\n",
        "\n",
        "        stgraph = dataset.get_stgraph(\n",
        "            max_frames=WINDOW_SIZE,\n",
        "            start_frame=start\n",
        "        )\n",
        "\n",
        "        node_features = stgraph[\"node_features\"].to(device)\n",
        "        adjacency = stgraph[\"adjacency\"].to(device)\n",
        "        fps = stgraph[\"fps\"]\n",
        "\n",
        "        edge_index = adjacency.nonzero(as_tuple=True)\n",
        "        edge_index = torch.stack(edge_index).long().to(device)\n",
        "\n",
        "        ppg_gt = dataset.load_ppg_ground_truth(num_frames=WINDOW_SIZE)\n",
        "        ppg_gt = torch.from_numpy(ppg_gt).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_waveform = trainer.model(node_features, edge_index)\n",
        "            pred_ppg = pred_waveform.cpu().numpy()\n",
        "\n",
        "        pred_hr = SignalProcessor.extract_heart_rate(pred_ppg, fps)\n",
        "        gt_hr = SignalProcessor.extract_heart_rate(ppg_gt.cpu().numpy(), fps)\n",
        "\n",
        "        print(f\"Pred HR: {pred_hr:.2f} BPM | GT HR: {gt_hr:.2f} BPM\")\n",
        "\n",
        "        all_pred_hr.append(pred_hr)\n",
        "        all_gt_hr.append(gt_hr)\n",
        "\n",
        "        start += STRIDE\n",
        "        window_id += 1\n",
        "\n",
        "    # ================= FINAL RESULTS =================\n",
        "    all_pred_hr = np.array(all_pred_hr)\n",
        "    all_gt_hr = np.array(all_gt_hr)\n",
        "\n",
        "    final_pred_hr = np.nanmean(all_pred_hr)\n",
        "    final_gt_hr = np.nanmean(all_gt_hr)\n",
        "    mae = np.nanmean(np.abs(all_pred_hr - all_gt_hr))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Final Video-Level Results\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Predicted HR (avg): {final_pred_hr:.2f} BPM\")\n",
        "    print(f\"Ground Truth HR (avg): {final_gt_hr:.2f} BPM\")\n",
        "    print(f\"MAE: {mae:.2f} BPM\")\n",
        "\n",
        "    print(\"\\nUBFC sliding-window evaluation complete!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-wGE66Saj4b",
        "outputId": "5cb62b17-76b1-4080-badc-8dd5422599fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MiniMeshPhys: UBFC Facial rPPG Evaluation (Sliding Window)\n",
            "======================================================================\n",
            "\n",
            "Device: cuda\n",
            "\n",
            "Model created with 220778 parameters\n",
            "\n",
            "Total video frames: 1386\n",
            "Window size: 450, Stride: 150\n",
            "\n",
            "======================================================================\n",
            "Training MiniMeshPhys on first window only\n",
            "======================================================================\n",
            "Processing frames 0 to 450\n",
            "Epoch  10 | Loss: 1.2337 | Pred HR: 68.0 BPM, GT HR: 56.0 BPM\n",
            "Epoch  20 | Loss: 0.9067 | Pred HR: 68.0 BPM, GT HR: 56.0 BPM\n",
            "Epoch  30 | Loss: 0.8143 | Pred HR: 68.0 BPM, GT HR: 56.0 BPM\n",
            "Epoch  40 | Loss: 0.6893 | Pred HR: 68.0 BPM, GT HR: 56.0 BPM\n",
            "Epoch  50 | Loss: 0.6660 | Pred HR: 68.0 BPM, GT HR: 56.0 BPM\n",
            "\n",
            "======================================================================\n",
            "Running sliding-window evaluation on full video\n",
            "======================================================================\n",
            "\n",
            "Window 1: frames 0 to 450\n",
            "Processing frames 0 to 450\n",
            "Pred HR: 72.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "Window 2: frames 150 to 600\n",
            "Processing frames 150 to 600\n",
            "Pred HR: 72.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "Window 3: frames 300 to 750\n",
            "Processing frames 300 to 750\n",
            "Pred HR: 52.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "Window 4: frames 450 to 900\n",
            "Processing frames 450 to 900\n",
            "Pred HR: 48.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "Window 5: frames 600 to 1050\n",
            "Processing frames 600 to 1050\n",
            "Pred HR: 56.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "Window 6: frames 750 to 1200\n",
            "Processing frames 750 to 1200\n",
            "Pred HR: 56.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "Window 7: frames 900 to 1350\n",
            "Processing frames 900 to 1350\n",
            "Pred HR: 52.00 BPM | GT HR: 56.00 BPM\n",
            "\n",
            "======================================================================\n",
            "Final Video-Level Results\n",
            "======================================================================\n",
            "Predicted HR (avg): 58.29 BPM\n",
            "Ground Truth HR (avg): 56.00 BPM\n",
            "MAE: 6.86 BPM\n",
            "\n",
            "UBFC sliding-window evaluation complete!\n"
          ]
        }
      ]
    }
  ]
}